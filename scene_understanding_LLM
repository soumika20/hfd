!pip install transformers pillow --quiet

import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image
import io

# Load SmolVLM
model_name = "HuggingFaceTB/SmolVLM-256M-Instruct"  

processor = AutoProcessor.from_pretrained(model_name)
model = AutoModelForVision2Seq.from_pretrained(
    model_name,
    torch_dtype=torch.float16,   
    device_map="auto"            
)

# Load image from file path
# image file location
img_path = "file.jpg"  
image = Image.open(img_path).convert("RGB")
print("Image loaded:", img_path)

# Build prompt with <image>
prompt = "<image> Describe the image in detail: list objects, scene, environment, and give a summary."

inputs = processor(
    text=[prompt],      
    images=[image],
    return_tensors="pt"
).to(model.device)

# Generate summary
output = model.generate(
    **inputs,
    max_new_tokens=200,
    do_sample=False
)

summary = processor.decode(output[0], skip_special_tokens=True)

print("\nIMAGE SUMMARY\n")
print(summary)
